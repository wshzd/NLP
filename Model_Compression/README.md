#模型压缩开山之作

http://papers.nips.cc/paper/250-optimal-brain-damage.pdf

##模型压缩综述

https://arxiv.org/abs/1510.00149.pdf

##基于核稀疏化的方法

《Learning Structured Sparsity in Deep Neural Networks》

http://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural-networks.pdf

https://github.com/wenwei202/caffe/tree/scnn

《Dynamic Network Surgery for Efficient DNNs》

http://arxiv.org/abs/1608.04493.pdf

https://github.com/yiwenguo/Dynamic-Network-Surgery

《Training Skinny Deep Neural Networks with Iterative Hard Thresholding Methods》

https://arxiv.org/abs/1607.05423.pdf

##基于模型裁剪的方法

《Pruning Filters for Efficient Convnets》

https://arxiv.org/pdf/1608.08710.pdf

《Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures》

https://arxiv.org/abs/1607.03250.pdf

《An Entropy-based Pruning Method for CNN Compression》

https://arxiv.org/pdf/1706.05791.pdf

《Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning》

https://arxiv.org/pdf/1611.05128.pdf

##基于迁移学习的方法

《Distilling the Knowledge in a Neural Network》

https://arxiv.org/pdf/1503.02531.pdf

《Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer》

https://arxiv.org/abs/1612.03928.pdf

https://github.com/szagoruyko/attention-transfer

##基于精细模型设计的方法

《MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications》

https://arxiv.org/abs/1704.04861.pdf

https://github.com/shicai/MobileNet-Caffe

《Aggregated Residual Transformations for Deep Neural Networks》

https://arxiv.org/pdf/1611.05431.pdf

https://github.com/facebookresearch/ResNeXt

《ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices》

https://arxiv.org/abs/1707.01083?context=cs.CV








