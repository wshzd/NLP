- 1706: [Transformer](https://arxiv.org/abs/1706.03762.pdf). [tensorflow version](https://github.com/tensorflow/tensor2tensor)[pytorch version](http://nlp.seas.harvard.edu/2018/04/03/attention.html)




* 2006: [Funnel-Transformer](http://github.com/laiguokun/Funnel-Transformer) [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) by Zihang Dai (CMU, Google), Guokun Lai (CMU), Yiming Yang (CMU), Quoc V. Le (Google)

* 2009: [Performer](https://github.com/google-research/google-research/tree/master/performer) [Rethinking Attention with Performers](https://arxiv.org/pdf/2009.14794)[official website](https://www.aminer.cn/pub/5f75feb191e0111c1eb4dbb2/rethinking-attention-with-performers) by Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou SongAndreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller

* 2012: [Informer]() [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/pdf/2012.07436.pdf)

* : []() [DeepNet_ Scaling Transformers to 1,000 Layers]()




