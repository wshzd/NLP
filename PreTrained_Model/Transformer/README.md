# Transformer
- 1706: [Transformer Paper](https://arxiv.org/abs/1706.03762.pdf). [tensorflow version](https://github.com/tensorflow/tensor2tensor) and [pytorch version](http://nlp.seas.harvard.edu/2018/04/03/attention.html)



# Transformer Variant
- 1901: [Transformer-XL](https://github.com/kimiyoung/transformer-xl) (from google) released with the paper [Transformer-XL: Attentive Language Models
Beyond a Fixed-Length Context](https://arxiv.org/pdf/1901.02860.pdf) by Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.
[CSDN blog](https://blog.csdn.net/magical_bubble/article/details/89060213)






* 2006: [Funnel-Transformer](http://github.com/laiguokun/Funnel-Transformer) [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) by Zihang Dai (CMU, Google), Guokun Lai (CMU), Yiming Yang (CMU), Quoc V. Le (Google)

* 2009: [Performer](https://github.com/google-research/google-research/tree/master/performer) [Rethinking Attention with Performers](https://arxiv.org/pdf/2009.14794)[official website](https://www.aminer.cn/pub/5f75feb191e0111c1eb4dbb2/rethinking-attention-with-performers) by Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou SongAndreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller

* 2012: [Informer]() [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/pdf/2012.07436.pdf)

* : []() [DeepNet_ Scaling Transformers to 1,000 Layers]()




