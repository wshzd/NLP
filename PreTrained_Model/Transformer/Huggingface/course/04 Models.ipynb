{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN5vEedlQ8WNDVT7P56dX4J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LwhDJ15xA6gv","executionInfo":{"status":"ok","timestamp":1675760489263,"user_tz":-480,"elapsed":11616,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}},"outputId":"fb18a320-5690-4c79-95a8-6b39a1cb004d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.8/dist-packages (4.26.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (2022.6.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (0.13.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (3.9.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (2.25.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (0.12.0)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (0.1.97)\n","Requirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (3.19.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers[sentencepiece]) (4.4.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[sentencepiece]) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[sentencepiece]) (4.0.0)\n"]}],"source":["!pip install transformers\n","!pip install transformers[sentencepiece]"]},{"cell_type":"markdown","source":["In this section we’ll take a closer look at creating and using a model. We’ll use the AutoModel class, which is handy when you want to instantiate any model from a checkpoint.\n","\n","The AutoModel class and all of its relatives are actually simple wrappers over the wide variety of models available in the library. It’s a clever wrapper as it can automatically guess the appropriate model architecture for your checkpoint, and then instantiates a model with this architecture.\n","\n","However, if you know the type of model you want to use, you can use the class that defines its architecture directly. Let’s take a look at how this works with a BERT model."],"metadata":{"id":"kiUQjm5rBl38"}},{"cell_type":"code","source":["# Method1： Creating a model from the default configuration initializes it with random values:\n","from transformers import BertConfig, BertModel\n","\n","config = BertConfig()\n","model = BertModel(config)\n","\n","# Model is randomly initialized!"],"metadata":{"id":"QhZaIEafBnKG","executionInfo":{"status":"ok","timestamp":1675760496763,"user_tz":-480,"elapsed":7507,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["print(config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ti7BRyaVCzjT","executionInfo":{"status":"ok","timestamp":1675760496768,"user_tz":-480,"elapsed":21,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}},"outputId":"cc77223f-adf3-4e7c-a270-412a0eba6e86"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.26.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n"]}]},{"cell_type":"code","source":["# Method2： Loading a Transformer model that is already trained is simple — we can do this using the from_pretrained() method:\n","from transformers import BertModel\n","\n","model = BertModel.from_pretrained(\"bert-base-cased\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JxPw_Q57C3A1","executionInfo":{"status":"ok","timestamp":1675760498746,"user_tz":-480,"elapsed":1991,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}},"outputId":"3a450867-0e20-4ed3-f345-2b0a5eb1ed52"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["## Saving methods\n","model.save_pretrained(\"directory_on_my_computer\")"],"metadata":{"id":"-ACprBrSDAPj","executionInfo":{"status":"ok","timestamp":1675760500098,"user_tz":-480,"elapsed":1359,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["%%bash\n","ls directory_on_my_computer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C5aFDFXkDmet","executionInfo":{"status":"ok","timestamp":1675760500099,"user_tz":-480,"elapsed":7,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}},"outputId":"2d797e18-3f27-41e9-d74a-78235e720b4b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["config.json\n","pytorch_model.bin\n"]}]},{"cell_type":"markdown","source":["If you take a look at the config.json file, you’ll recognize the attributes necessary to build the model architecture. This file also contains some metadata, such as where the checkpoint originated and what 🤗 Transformers version you were using when you last saved the checkpoint.\n","\n","The pytorch_model.bin file is known as the state dictionary; it contains all your model’s weights. The two files go hand in hand; the configuration is necessary to know your model’s architecture, while the model weights are your model’s parameters."],"metadata":{"id":"OvZFTAyvEGTu"}},{"cell_type":"code","source":["# Using a Transformer model for inference\n","# Let’s say we have a couple of sequences:\n","sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"],"metadata":{"id":"uvi2_10YDv-r","executionInfo":{"status":"ok","timestamp":1675760500099,"user_tz":-480,"elapsed":5,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# The tokenizer converts these to vocabulary indices which are typically called input IDs. Each sequence is now a list of numbers! The resulting output is:\n","encoded_sequences = [\n","    [101, 7592, 999, 102],\n","    [101, 4658, 1012, 102],\n","    [101, 3835, 999, 102],\n","]"],"metadata":{"id":"Ycen0EqwFXM4","executionInfo":{"status":"ok","timestamp":1675760500100,"user_tz":-480,"elapsed":5,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# This is a list of encoded sequences: a list of lists. Tensors only accept rectangular shapes (think matrices). This “array” is already of rectangular shape, so converting it to a tensor is easy:\n","import torch\n","\n","model_inputs = torch.tensor(encoded_sequences)"],"metadata":{"id":"Dm-1RJzEGNLw","executionInfo":{"status":"ok","timestamp":1675760500581,"user_tz":-480,"elapsed":486,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Using the tensors as inputs to the model\n","output = model(model_inputs)"],"metadata":{"id":"s4HbKUMUGNwn","executionInfo":{"status":"ok","timestamp":1675760500582,"user_tz":-480,"elapsed":5,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}}},"execution_count":10,"outputs":[]}]}