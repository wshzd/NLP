{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMna2mIxTbwLU20EAK49CGf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["The most basic object in the ü§ó Transformers library is the pipeline() function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:"],"metadata":{"id":"yq-kj5Y5xNKO"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install transformers[sentencepiece]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G7oNK_1Hx18s","executionInfo":{"status":"ok","timestamp":1675756456806,"user_tz":-480,"elapsed":10529,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}},"outputId":"2cfa03e5-e660-4f4c-fd11-2d4fe1da420c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.8/dist-packages (4.26.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (4.64.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (0.12.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (3.9.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (1.21.6)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (0.13.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (2022.6.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (2.25.1)\n","Requirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (3.19.6)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (0.1.97)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers[sentencepiece]) (4.4.0)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[sentencepiece]) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[sentencepiece]) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n"]}]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","classifier = pipeline(\"sentiment-analysis\")\n","classifier(\"I've been waiting for a HuggingFace course my whole life.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vXoGB0kbxNwj","executionInfo":{"status":"ok","timestamp":1675756470843,"user_tz":-480,"elapsed":14048,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}},"outputId":"206a7ca4-8cea-4892-b3b3-d564f68e8bd0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9598049521446228}]"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["We can even pass several sentences!"],"metadata":{"id":"QXGi192mynVQ"}},{"cell_type":"code","source":["classifier(\n","    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Zi4_QSSyoLd","executionInfo":{"status":"ok","timestamp":1675756470844,"user_tz":-480,"elapsed":30,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}},"outputId":"5c96aeab-87df-4efe-aed6-8346ebc2a843"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n"," {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# Zero-shot classification\n","from transformers import pipeline\n","\n","classifier = pipeline(\"zero-shot-classification\")\n","classifier(\n","    \"This is a course about the Transformers library\",\n","    candidate_labels=[\"education\", \"politics\", \"business\"],\n",")\n","# This pipeline is called zero-shot because you don‚Äôt need to fine-tune the model on your data to use it. It can directly return probability scores for any list of labels you want!"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xyksl_soyrLl","executionInfo":{"status":"ok","timestamp":1675756492985,"user_tz":-480,"elapsed":22166,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}},"outputId":"96191b5c-6b43-4c0a-914e-695f9af5af04"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"output_type":"execute_result","data":{"text/plain":["{'sequence': 'This is a course about the Transformers library',\n"," 'labels': ['education', 'business', 'politics'],\n"," 'scores': [0.8445988297462463, 0.11197440326213837, 0.04342682659626007]}"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# Text generation\n","from transformers import pipeline\n","\n","generator = pipeline(\"text-generation\")\n","generator(\"In this course, we will teach you how to\")\n","# You can control how many different sequences are generated with the argument num_return_sequences and the total length of the output text with the argument max_length."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rts7dqkRzkL9","executionInfo":{"status":"ok","timestamp":1675756501618,"user_tz":-480,"elapsed":8660,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}},"outputId":"056da05b-613e-4d78-f064-a5ca1cffc82e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n","  warnings.warn(\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 50 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': 'In this course, we will teach you how to create, manage, and maintain a company, as well as demonstrate an understanding of the most critical aspects of each.\\n\\nThe course is divided into four segments. Each segment consists of five topics.'}]"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Text generation Using any model from the Hub in a pipeline\n","from transformers import pipeline\n","\n","generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n","generator(\n","    \"In this course, we will teach you how to\",\n","    max_length=30,\n","    num_return_sequences=2,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3k1zhA10cTC","executionInfo":{"status":"ok","timestamp":1675756507401,"user_tz":-480,"elapsed":5820,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}},"outputId":"9fff60f1-4b28-4097-bcea-f77d04952aaf"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': 'In this course, we will teach you how to perform the exercises which you will find useful in the course. It also gives you a more detailed knowledge'},\n"," {'generated_text': 'In this course, we will teach you how to learn to run and run. Here are some tips:\\n\\n\\n\\n\\n\\n\\n\\n\\n'}]"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# Mask filling\n","# The next pipeline you‚Äôll try is fill-mask. The idea of this task is to fill in the blanks in a given text:\n","from transformers import pipeline\n","\n","unmasker = pipeline(\"fill-mask\")\n","unmasker(\"This course will teach you all about <mask> models.\", top_k=2)\n","# The top_k argument controls how many possibilities you want to be displayed. Note that here the model fills in the special <mask> word, which is often referred to as a mask token. Other mask-filling models might have different mask tokens, so it‚Äôs always good to verify the proper mask word when exploring other models. One way to check it is by looking at the mask word used in the widget."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xFYjalQ-07Rc","executionInfo":{"status":"ok","timestamp":1675756510697,"user_tz":-480,"elapsed":3309,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}},"outputId":"8cf9427d-ce92-4011-e6eb-8401a8e54081"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.19619810581207275,\n","  'token': 30412,\n","  'token_str': ' mathematical',\n","  'sequence': 'This course will teach you all about mathematical models.'},\n"," {'score': 0.04052736610174179,\n","  'token': 38163,\n","  'token_str': ' computational',\n","  'sequence': 'This course will teach you all about computational models.'}]"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# Named entity recognition\n","from transformers import pipeline\n","\n","ner = pipeline(\"ner\", grouped_entities=True)\n","ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n","# We pass the option grouped_entities=True in the pipeline creation function to tell the pipeline to regroup together the parts of the sentence that correspond to the same entity: here the model correctly grouped ‚ÄúHugging‚Äù and ‚ÄúFace‚Äù as a single organization, even though the name consists of multiple words. In fact, as we will see in the next chapter, the preprocessing even splits some words into smaller parts. For instance, Sylvain is split into four pieces: S, ##yl, ##va, and ##in. In the post-processing step, the pipeline successfully regrouped those pieces."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aom9UM2n1TvK","executionInfo":{"status":"ok","timestamp":1675756522064,"user_tz":-480,"elapsed":11373,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}},"outputId":"83489583-974a-4ff7-e83f-0ab77f9e4bac"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","/usr/local/lib/python3.8/dist-packages/transformers/pipelines/token_classification.py:159: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'entity_group': 'PER',\n","  'score': 0.9981694,\n","  'word': 'Sylvain',\n","  'start': 11,\n","  'end': 18},\n"," {'entity_group': 'ORG',\n","  'score': 0.9796019,\n","  'word': 'Hugging Face',\n","  'start': 33,\n","  'end': 45},\n"," {'entity_group': 'LOC',\n","  'score': 0.9932106,\n","  'word': 'Brooklyn',\n","  'start': 49,\n","  'end': 57}]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# Question answering\n","# The question-answering pipeline answers questions using information from a given context:\n","from transformers import pipeline\n","\n","question_answerer = pipeline(\"question-answering\")\n","question_answerer(\n","    question=\"Where do I work?\",\n","    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JiVHHo_s11kW","executionInfo":{"status":"ok","timestamp":1675756525195,"user_tz":-480,"elapsed":3139,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}},"outputId":"542d891b-816b-4049-b223-6a1c4685f497"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"output_type":"execute_result","data":{"text/plain":["{'score': 0.6949766278266907, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Summarization\n","# Summarization is the task of reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text. Here‚Äôs an example:\n","from transformers import pipeline\n","\n","summarizer = pipeline(\"summarization\")\n","summarizer(\n","    \"\"\"\n","    America has changed dramatically during recent years. Not only has the number of \n","    graduates in traditional engineering disciplines such as mechanical, civil, \n","    electrical, chemical, and aeronautical engineering declined, but in most of \n","    the premier American universities engineering curricula now concentrate on \n","    and encourage largely the study of engineering science. As a result, there \n","    are declining offerings in engineering subjects dealing with infrastructure, \n","    the environment, and related issues, and greater concentration on high \n","    technology subjects, largely supporting increasingly complex scientific \n","    developments. While the latter is important, it should not be at the expense \n","    of more traditional engineering.\n","\n","    Rapidly developing economies such as China and India, as well as other \n","    industrial countries in Europe and Asia, continue to encourage and advance \n","    the teaching of engineering. Both China and India, respectively, graduate \n","    six and eight times as many traditional engineers as does the United States. \n","    Other industrial countries at minimum maintain their output, while America \n","    suffers an increasingly serious decline in the number of engineering graduates \n","    and a lack of well-educated engineers.\n","\"\"\"\n",")\n","# Like with text generation, you can specify a max_length or a min_length for the result."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E2vSSz6s2Dyb","executionInfo":{"status":"ok","timestamp":1675756548258,"user_tz":-480,"elapsed":23075,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}},"outputId":"4aa23f7f-2cab-4e35-99ea-99c82ad7aa06"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# Translation\n","# For translation, you can use a default model if you provide a language pair in the task name (such as \"translation_en_to_fr\"), but the easiest way is to pick the model you want to use on the Model Hub. Here we‚Äôll try translating from French to English:\n","from transformers import pipeline\n","\n","translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n","translator(\"Ce cours est produit par Hugging Face.\")\n","# Like with text generation and summarization, you can specify a max_length or a min_length for the result."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R9xRtvbf2XYu","executionInfo":{"status":"ok","timestamp":1675756554953,"user_tz":-480,"elapsed":6711,"user":{"displayName":"w-s-h-z-d@163.com","userId":"06288773698198803165"}},"outputId":"449afe00-d481-4136-d3d5-b873b7924e46"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n","  warnings.warn(\"Recommended: pip install sacremoses.\")\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'translation_text': 'This course is produced by Hugging Face.'}]"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["Huggingface pipeline model VS task refer to https://huggingface.co/models"],"metadata":{"id":"ctfArBYZ3R-g"}}]}