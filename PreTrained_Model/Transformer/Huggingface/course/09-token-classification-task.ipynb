{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install transformers[sentencepiece]\n!pip install datasets\n!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:02:28.747792Z","iopub.execute_input":"2023-02-08T10:02:28.748300Z","iopub.status.idle":"2023-02-08T10:03:16.359630Z","shell.execute_reply.started":"2023-02-08T10:02:28.748197Z","shell.execute_reply":"2023-02-08T10:03:16.358205Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: transformers[sentencepiece] in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (1.21.6)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (2.28.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (6.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (3.7.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (6.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (4.64.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.10.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.12.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (2021.11.10)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (23.0)\nRequirement already satisfied: protobuf<=3.20.1 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (3.19.4)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.1.97)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]) (4.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers[sentencepiece]) (3.8.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (1.26.14)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2023.1.0)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.10.1)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (5.0.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (23.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.64.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (6.0.0)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.1.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: evaluate in /opt/conda/lib/python3.7/site-packages (0.4.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from evaluate) (3.2.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.10.1)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.3.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from evaluate) (23.0)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2023.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.21.6)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.28.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.70.14)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.3.5)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.64.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from evaluate) (6.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (3.8.1)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (5.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.7.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2.1.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->evaluate) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2022.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.4.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.13.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Token classification Task as follows:\n**Named entity recognition (NER)**: Find the entities (such as persons, locations, or organizations) in a sentence. This can be formulated as attributing a label to each token by having one class per entity and one class for “no entity.”  \n**Part-of-speech tagging (POS)**: Mark each word in a sentence as corresponding to a particular part of speech (such as noun, verb, adjective, etc.).  \n**Chunking**: Find the tokens that belong to the same entity. This task (which can be combined with POS or NER) can be formulated as attributing one label (usually B-) to any tokens that are at the beginning of a chunk, another label (usually I-) to tokens that are inside a chunk, and a third label (usually O) to tokens that don’t belong to any chunk.","metadata":{}},{"cell_type":"markdown","source":"# we will fine-tune a model (BERT) on a NER task","metadata":{}},{"cell_type":"code","source":"# Preparing the data\n# First things first, we need a dataset suitable for token classification. In this section we will use the CoNLL-2003 dataset, which contains news stories from Reuters.\nfrom datasets import load_dataset\n\nraw_datasets = load_dataset(\"conll2003\")","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:16.362864Z","iopub.execute_input":"2023-02-08T10:03:16.363354Z","iopub.status.idle":"2023-02-08T10:03:17.791427Z","shell.execute_reply.started":"2023-02-08T10:03:16.363304Z","shell.execute_reply":"2023-02-08T10:03:17.789917Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee258aefd57542179de28d9679f193a6"}},"metadata":{}}]},{"cell_type":"code","source":"raw_datasets","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:17.793806Z","iopub.execute_input":"2023-02-08T10:03:17.795772Z","iopub.status.idle":"2023-02-08T10:03:17.809147Z","shell.execute_reply.started":"2023-02-08T10:03:17.795695Z","shell.execute_reply":"2023-02-08T10:03:17.807670Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 14042\n    })\n    validation: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 3251\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 3454\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Let’s have a look at the first element of the training set:\nraw_datasets[\"train\"][0][\"tokens\"]","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:17.814118Z","iopub.execute_input":"2023-02-08T10:03:17.815238Z","iopub.status.idle":"2023-02-08T10:03:17.826177Z","shell.execute_reply.started":"2023-02-08T10:03:17.815188Z","shell.execute_reply":"2023-02-08T10:03:17.824720Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"},"metadata":{}}]},{"cell_type":"code","source":"# Since we want to perform named entity recognition, we will look at the NER tags:\nraw_datasets[\"train\"][0][\"ner_tags\"]","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:17.828515Z","iopub.execute_input":"2023-02-08T10:03:17.829970Z","iopub.status.idle":"2023-02-08T10:03:17.839541Z","shell.execute_reply.started":"2023-02-08T10:03:17.829922Z","shell.execute_reply":"2023-02-08T10:03:17.838067Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[3, 0, 7, 0, 0, 0, 7, 0, 0]"},"metadata":{}}]},{"cell_type":"markdown","source":"Those are the labels as integers ready for training, but they’re not necessarily useful when we want to inspect the data. Like for text classification, we can access the correspondence between those integers and the label names by looking at the features attribute of our dataset:","metadata":{}},{"cell_type":"code","source":"ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\nner_feature","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:17.841922Z","iopub.execute_input":"2023-02-08T10:03:17.843078Z","iopub.status.idle":"2023-02-08T10:03:17.853987Z","shell.execute_reply.started":"2023-02-08T10:03:17.843025Z","shell.execute_reply":"2023-02-08T10:03:17.852471Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"},"metadata":{}}]},{"cell_type":"code","source":"# The type of the elements of the sequence is in the feature attribute of this ner_feature\nlabel_names = ner_feature.feature.names\nlabel_names","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:17.856307Z","iopub.execute_input":"2023-02-08T10:03:17.858014Z","iopub.status.idle":"2023-02-08T10:03:17.867929Z","shell.execute_reply.started":"2023-02-08T10:03:17.857948Z","shell.execute_reply":"2023-02-08T10:03:17.866321Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"},"metadata":{}}]},{"cell_type":"markdown","source":"O means the word doesn’t correspond to any entity.  \nB-PER/I-PER means the word corresponds to the beginning of/is inside a person entity.  \nB-ORG/I-ORG means the word corresponds to the beginning of/is inside an organization entity.  \nB-LOC/I-LOC means the word corresponds to the beginning of/is inside a location entity.  \nB-MISC/I-MISC means the word corresponds to the beginning of/is inside a miscellaneous entity.  ","metadata":{}},{"cell_type":"code","source":"# Now decoding the labels we saw earlier gives us this:\nwords = raw_datasets[\"train\"][0][\"tokens\"]\nlabels = raw_datasets[\"train\"][0][\"ner_tags\"]\nline1 = \"\"\nline2 = \"\"\nfor word, label in zip(words, labels):\n    full_label = label_names[label]\n    max_length = max(len(word), len(full_label))\n    line1 += word + \" \" * (max_length - len(word) + 1)\n    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n\nprint(line1)\nprint(line2)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:17.870153Z","iopub.execute_input":"2023-02-08T10:03:17.871103Z","iopub.status.idle":"2023-02-08T10:03:17.882659Z","shell.execute_reply.started":"2023-02-08T10:03:17.871053Z","shell.execute_reply":"2023-02-08T10:03:17.881175Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"EU    rejects German call to boycott British lamb . \nB-ORG O       B-MISC O    O  O       B-MISC  O    O \n","output_type":"stream"}]},{"cell_type":"code","source":"# let’s create our tokenizer object\nfrom transformers import AutoTokenizer\n\nmodel_checkpoint = \"bert-base-cased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:17.885053Z","iopub.execute_input":"2023-02-08T10:03:17.885964Z","iopub.status.idle":"2023-02-08T10:03:21.244336Z","shell.execute_reply.started":"2023-02-08T10:03:17.885909Z","shell.execute_reply":"2023-02-08T10:03:21.243157Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# To tokenize a pre-tokenized input, we can use our tokenizer as usual and just add is_split_into_words=True:\ninputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\ninputs.tokens()","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:21.250237Z","iopub.execute_input":"2023-02-08T10:03:21.250689Z","iopub.status.idle":"2023-02-08T10:03:21.261196Z","shell.execute_reply.started":"2023-02-08T10:03:21.250619Z","shell.execute_reply":"2023-02-08T10:03:21.259830Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"['[CLS]',\n 'EU',\n 'rejects',\n 'German',\n 'call',\n 'to',\n 'boycott',\n 'British',\n 'la',\n '##mb',\n '.',\n '[SEP]']"},"metadata":{}}]},{"cell_type":"code","source":"# because we’re using a fast tokenizer we have access to the 🤗 Tokenizers superpowers, which means we can easily map each token to its corresponding word\ninputs.word_ids()","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:21.263051Z","iopub.execute_input":"2023-02-08T10:03:21.264344Z","iopub.status.idle":"2023-02-08T10:03:21.272793Z","shell.execute_reply.started":"2023-02-08T10:03:21.264298Z","shell.execute_reply":"2023-02-08T10:03:21.271487Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"},"metadata":{}}]},{"cell_type":"code","source":"# The first rule we’ll apply is that special tokens get a label of -100. This is because by default -100 is an index that is ignored in the loss function we will use (cross entropy)\ndef align_labels_with_tokens(labels, word_ids):\n    new_labels = []\n    current_word = None\n    for word_id in word_ids:\n        if word_id != current_word:\n            # Start of a new word!\n            current_word = word_id\n            label = -100 if word_id is None else labels[word_id]\n            new_labels.append(label)\n        elif word_id is None:\n            # Special token\n            new_labels.append(-100)\n        else:\n            # Same word as previous token\n            label = labels[word_id]\n            # If the label is B-XXX we change it to I-XXX\n            if label % 2 == 1:\n                label += 1\n            new_labels.append(label)\n\n    return new_labels","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:21.274684Z","iopub.execute_input":"2023-02-08T10:03:21.275889Z","iopub.status.idle":"2023-02-08T10:03:21.286465Z","shell.execute_reply.started":"2023-02-08T10:03:21.275842Z","shell.execute_reply":"2023-02-08T10:03:21.285155Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"labels = raw_datasets[\"train\"][0][\"ner_tags\"]\nword_ids = inputs.word_ids()\nprint(labels)\nprint(align_labels_with_tokens(labels, word_ids))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:21.288188Z","iopub.execute_input":"2023-02-08T10:03:21.289101Z","iopub.status.idle":"2023-02-08T10:03:21.300616Z","shell.execute_reply.started":"2023-02-08T10:03:21.289045Z","shell.execute_reply":"2023-02-08T10:03:21.299439Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[3, 0, 7, 0, 0, 0, 7, 0, 0]\n[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"To take advantage of the speed of our fast tokenizer, it’s best to tokenize lots of texts at the same time, so we’ll write a function that processes a list of examples and use the Dataset.map() method with the option batched=True. ","metadata":{}},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"], truncation=True, is_split_into_words=True\n    )\n    all_labels = examples[\"ner_tags\"]\n    new_labels = []\n    for i, labels in enumerate(all_labels):\n        word_ids = tokenized_inputs.word_ids(i)\n        new_labels.append(align_labels_with_tokens(labels, word_ids))\n\n    tokenized_inputs[\"labels\"] = new_labels\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:21.302690Z","iopub.execute_input":"2023-02-08T10:03:21.303519Z","iopub.status.idle":"2023-02-08T10:03:21.311133Z","shell.execute_reply.started":"2023-02-08T10:03:21.303473Z","shell.execute_reply":"2023-02-08T10:03:21.310083Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = raw_datasets.map(\n    tokenize_and_align_labels,\n    batched=True,\n    remove_columns=raw_datasets[\"train\"].column_names,\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:21.313007Z","iopub.execute_input":"2023-02-08T10:03:21.313843Z","iopub.status.idle":"2023-02-08T10:03:25.221465Z","shell.execute_reply.started":"2023-02-08T10:03:21.313796Z","shell.execute_reply":"2023-02-08T10:03:25.220166Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d5c5dde468c4e6c8182ebeeed0f9784"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7de7175e45e94b44afb2c5769b399b97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7caa3ad1459d4d88afe2f0908e0575bc"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Fine-tuning the model with the Trainer API","metadata":{}},{"cell_type":"code","source":"# Data collation\nfrom transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:25.223286Z","iopub.execute_input":"2023-02-08T10:03:25.224026Z","iopub.status.idle":"2023-02-08T10:03:30.307346Z","shell.execute_reply.started":"2023-02-08T10:03:25.223977Z","shell.execute_reply":"2023-02-08T10:03:30.304701Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# To test this on a few samples, we can just call it on a list of examples from our tokenized training set:\nbatch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\nbatch[\"labels\"]","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:30.311150Z","iopub.execute_input":"2023-02-08T10:03:30.312374Z","iopub.status.idle":"2023-02-08T10:03:30.331284Z","shell.execute_reply.started":"2023-02-08T10:03:30.312320Z","shell.execute_reply":"2023-02-08T10:03:30.327589Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])"},"metadata":{}}]},{"cell_type":"code","source":"for i in range(2):\n    print(tokenized_datasets[\"train\"][i][\"labels\"])","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:30.336846Z","iopub.execute_input":"2023-02-08T10:03:30.342949Z","iopub.status.idle":"2023-02-08T10:03:30.368142Z","shell.execute_reply.started":"2023-02-08T10:03:30.342892Z","shell.execute_reply":"2023-02-08T10:03:30.365949Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n[-100, 1, 2, -100]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Metrics","metadata":{}},{"cell_type":"code","source":"# The traditional framework used to evaluate token classification prediction is seqeval. To use this metric, we first need to install the seqeval library:\n!pip install seqeval","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:30.369963Z","iopub.execute_input":"2023-02-08T10:03:30.370397Z","iopub.status.idle":"2023-02-08T10:03:42.224014Z","shell.execute_reply.started":"2023-02-08T10:03:30.370355Z","shell.execute_reply":"2023-02-08T10:03:42.222611Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: seqeval in /opt/conda/lib/python3.7/site-packages (1.2.2)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval) (1.0.2)\nRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval) (1.21.6)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\n\nmetric = evaluate.load(\"seqeval\")","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:42.226569Z","iopub.execute_input":"2023-02-08T10:03:42.227083Z","iopub.status.idle":"2023-02-08T10:03:43.902299Z","shell.execute_reply.started":"2023-02-08T10:03:42.227032Z","shell.execute_reply":"2023-02-08T10:03:43.901011Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"This metric does not behave like the standard accuracy: it will actually take the lists of labels as strings, not integers, so we will need to fully decode the predictions and labels before passing them to the metric. Let’s see how it works. First, we’ll get the labels for our first training example:","metadata":{}},{"cell_type":"code","source":"labels = raw_datasets[\"train\"][0][\"ner_tags\"]\nlabels = [label_names[i] for i in labels]\nlabels","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:43.904066Z","iopub.execute_input":"2023-02-08T10:03:43.904529Z","iopub.status.idle":"2023-02-08T10:03:43.917123Z","shell.execute_reply.started":"2023-02-08T10:03:43.904480Z","shell.execute_reply":"2023-02-08T10:03:43.915752Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"},"metadata":{}}]},{"cell_type":"code","source":"# We can then create fake predictions for those by just changing the value at index 2:\npredictions = labels.copy()\npredictions[2] = \"O\"\nmetric.compute(predictions=[predictions], references=[labels])","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:43.919661Z","iopub.execute_input":"2023-02-08T10:03:43.920622Z","iopub.status.idle":"2023-02-08T10:03:43.950331Z","shell.execute_reply.started":"2023-02-08T10:03:43.920515Z","shell.execute_reply":"2023-02-08T10:03:43.947707Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'MISC': {'precision': 1.0,\n  'recall': 0.5,\n  'f1': 0.6666666666666666,\n  'number': 2},\n 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n 'overall_precision': 1.0,\n 'overall_recall': 0.6666666666666666,\n 'overall_f1': 0.8,\n 'overall_accuracy': 0.8888888888888888}"},"metadata":{}}]},{"cell_type":"markdown","source":"This compute_metrics() function first takes the argmax of the logits to convert them to predictions (as usual, the logits and the probabilities are in the same order, so we don’t need to apply the softmax). Then we have to convert both labels and predictions from integers to strings. We remove all the values where the label is -100, then pass the results to the metric.compute() method:","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n\ndef compute_metrics(eval_preds):\n    logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n\n    # Remove ignored index (special tokens) and convert to labels\n    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n    true_predictions = [\n        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": all_metrics[\"overall_precision\"],\n        \"recall\": all_metrics[\"overall_recall\"],\n        \"f1\": all_metrics[\"overall_f1\"],\n        \"accuracy\": all_metrics[\"overall_accuracy\"],\n    }","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:43.953317Z","iopub.execute_input":"2023-02-08T10:03:43.954399Z","iopub.status.idle":"2023-02-08T10:03:43.973340Z","shell.execute_reply.started":"2023-02-08T10:03:43.954311Z","shell.execute_reply":"2023-02-08T10:03:43.971084Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Defining the model","metadata":{}},{"cell_type":"code","source":"id2label = {i: label for i, label in enumerate(label_names)}\nlabel2id = {v: k for k, v in id2label.items()}","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:43.975908Z","iopub.execute_input":"2023-02-08T10:03:43.976741Z","iopub.status.idle":"2023-02-08T10:03:43.983971Z","shell.execute_reply.started":"2023-02-08T10:03:43.976690Z","shell.execute_reply":"2023-02-08T10:03:43.981965Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification\n\nmodel = AutoModelForTokenClassification.from_pretrained(\n    model_checkpoint,\n    id2label=id2label,\n    label2id=label2id,\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:43.987059Z","iopub.execute_input":"2023-02-08T10:03:43.988010Z","iopub.status.idle":"2023-02-08T10:03:47.323906Z","shell.execute_reply.started":"2023-02-08T10:03:43.987959Z","shell.execute_reply":"2023-02-08T10:03:47.322332Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.config.num_labels","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:47.326349Z","iopub.execute_input":"2023-02-08T10:03:47.326853Z","iopub.status.idle":"2023-02-08T10:03:47.336330Z","shell.execute_reply.started":"2023-02-08T10:03:47.326801Z","shell.execute_reply":"2023-02-08T10:03:47.334879Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"9"},"metadata":{}}]},{"cell_type":"markdown","source":"# Fine-tuning the model","metadata":{}},{"cell_type":"code","source":"# If you’re working in a notebook, there’s a convenience function to help you with this:\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n\n# If you aren’t working in a notebook, just type the following line in your terminal:\n#huggingface-cli login","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:47.338618Z","iopub.execute_input":"2023-02-08T10:03:47.339146Z","iopub.status.idle":"2023-02-08T10:03:47.393666Z","shell.execute_reply.started":"2023-02-08T10:03:47.339102Z","shell.execute_reply":"2023-02-08T10:03:47.392437Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e874f39992e4436b51aa1fa6aaf2816"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    \"bert-finetuned-ner\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    push_to_hub=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:47.402577Z","iopub.execute_input":"2023-02-08T10:03:47.403702Z","iopub.status.idle":"2023-02-08T10:03:47.478328Z","shell.execute_reply.started":"2023-02-08T10:03:47.403654Z","shell.execute_reply":"2023-02-08T10:03:47.477164Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:03:47.480329Z","iopub.execute_input":"2023-02-08T10:03:47.480825Z","iopub.status.idle":"2023-02-08T10:15:02.663864Z","shell.execute_reply.started":"2023-02-08T10:03:47.480783Z","shell.execute_reply":"2023-02-08T10:15:02.661179Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"/kaggle/working/bert-finetuned-ner is already a clone of https://huggingface.co/Arron/bert-finetuned-ner. Make sure you pull the latest changes with `repo.git_pull()`.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14042\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 5268\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mw-s-h-z-d\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.13.10"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230208_100358-u1kz568q</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/w-s-h-z-d/huggingface/runs/u1kz568q' target=\"_blank\">bert-finetuned-ner</a></strong> to <a href='https://wandb.ai/w-s-h-z-d/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/w-s-h-z-d/huggingface' target=\"_blank\">https://wandb.ai/w-s-h-z-d/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/w-s-h-z-d/huggingface/runs/u1kz568q' target=\"_blank\">https://wandb.ai/w-s-h-z-d/huggingface/runs/u1kz568q</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5268/5268 10:32, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.084500</td>\n      <td>0.062636</td>\n      <td>0.914606</td>\n      <td>0.933692</td>\n      <td>0.924051</td>\n      <td>0.982693</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.041400</td>\n      <td>0.056099</td>\n      <td>0.932077</td>\n      <td>0.949175</td>\n      <td>0.940549</td>\n      <td>0.986063</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.019800</td>\n      <td>0.060736</td>\n      <td>0.931946</td>\n      <td>0.949512</td>\n      <td>0.940647</td>\n      <td>0.986048</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 3251\n  Batch size = 8\nSaving model checkpoint to bert-finetuned-ner/checkpoint-1756\nConfiguration saved in bert-finetuned-ner/checkpoint-1756/config.json\nModel weights saved in bert-finetuned-ner/checkpoint-1756/pytorch_model.bin\ntokenizer config file saved in bert-finetuned-ner/checkpoint-1756/tokenizer_config.json\nSpecial tokens file saved in bert-finetuned-ner/checkpoint-1756/special_tokens_map.json\ntokenizer config file saved in bert-finetuned-ner/tokenizer_config.json\nSpecial tokens file saved in bert-finetuned-ner/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 3251\n  Batch size = 8\nSaving model checkpoint to bert-finetuned-ner/checkpoint-3512\nConfiguration saved in bert-finetuned-ner/checkpoint-3512/config.json\nModel weights saved in bert-finetuned-ner/checkpoint-3512/pytorch_model.bin\ntokenizer config file saved in bert-finetuned-ner/checkpoint-3512/tokenizer_config.json\nSpecial tokens file saved in bert-finetuned-ner/checkpoint-3512/special_tokens_map.json\ntokenizer config file saved in bert-finetuned-ner/tokenizer_config.json\nSpecial tokens file saved in bert-finetuned-ner/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 3251\n  Batch size = 8\nSaving model checkpoint to bert-finetuned-ner/checkpoint-5268\nConfiguration saved in bert-finetuned-ner/checkpoint-5268/config.json\nModel weights saved in bert-finetuned-ner/checkpoint-5268/pytorch_model.bin\ntokenizer config file saved in bert-finetuned-ner/checkpoint-5268/tokenizer_config.json\nSpecial tokens file saved in bert-finetuned-ner/checkpoint-5268/special_tokens_map.json\ntokenizer config file saved in bert-finetuned-ner/tokenizer_config.json\nSpecial tokens file saved in bert-finetuned-ner/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5268, training_loss=0.06633575277610901, metrics={'train_runtime': 666.9508, 'train_samples_per_second': 63.162, 'train_steps_per_second': 7.899, 'total_flos': 920831298449616.0, 'train_loss': 0.06633575277610901, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.push_to_hub(commit_message=\"Training complete\")","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:15:02.669765Z","iopub.execute_input":"2023-02-08T10:15:02.675077Z","iopub.status.idle":"2023-02-08T10:17:03.808499Z","shell.execute_reply.started":"2023-02-08T10:15:02.675016Z","shell.execute_reply":"2023-02-08T10:17:03.806917Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"Saving model checkpoint to bert-finetuned-ner\nConfiguration saved in bert-finetuned-ner/config.json\nModel weights saved in bert-finetuned-ner/pytorch_model.bin\ntokenizer config file saved in bert-finetuned-ner/tokenizer_config.json\nSpecial tokens file saved in bert-finetuned-ner/special_tokens_map.json\nSeveral commits (2) will be pushed upstream.\nThe progress bars may be unreliable.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Upload file pytorch_model.bin:   0%|          | 32.0k/411M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92177890252348a28615eae136db1043"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload file runs/Feb08_10-03-47_14c0196b45b7/events.out.tfevents.1675850635.14c0196b45b7.6593.0: 100%|########…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"763ee4644cda4f748f34b8a5c7f1c626"}},"metadata":{}},{"name":"stderr","text":"remote: Scanning LFS files for validity...        \nremote: LFS file scan complete.        \nTo https://huggingface.co/Arron/bert-finetuned-ner\n   bb50ff3..d26a63d  main -> main\n\nTo https://huggingface.co/Arron/bert-finetuned-ner\n   d26a63d..d902721  main -> main\n\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"'https://huggingface.co/Arron/bert-finetuned-ner/commit/d26a63d9b4c7cf581f535f1c287fd1430071942b'"},"metadata":{}}]},{"cell_type":"markdown","source":"# A custom training loop","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(\n    tokenized_datasets[\"train\"],\n    shuffle=True,\n    collate_fn=data_collator,\n    batch_size=8,\n)\neval_dataloader = DataLoader(\n    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:17:03.815137Z","iopub.execute_input":"2023-02-08T10:17:03.816235Z","iopub.status.idle":"2023-02-08T10:17:03.826275Z","shell.execute_reply.started":"2023-02-08T10:17:03.816191Z","shell.execute_reply":"2023-02-08T10:17:03.824759Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(\n    model_checkpoint,\n    id2label=id2label,\n    label2id=label2id,\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:17:03.829665Z","iopub.execute_input":"2023-02-08T10:17:03.830519Z","iopub.status.idle":"2023-02-08T10:17:11.006940Z","shell.execute_reply.started":"2023-02-08T10:17:03.830458Z","shell.execute_reply":"2023-02-08T10:17:11.005628Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"Exception in thread SystemMonitor:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 118, in _start\n    asset.start()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/system/assets/cpu.py\", line 166, in start\n    self.metrics_monitor.start()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 168, in start\n    logger.info(f\"Started {self._process.name}\")\nAttributeError: 'NoneType' object has no attribute 'name'\n\nloading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\nModel config BertConfig {\n  \"_name_or_path\": \"bert-base-cased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"O\",\n    \"1\": \"B-PER\",\n    \"2\": \"I-PER\",\n    \"3\": \"B-ORG\",\n    \"4\": \"I-ORG\",\n    \"5\": \"B-LOC\",\n    \"6\": \"I-LOC\",\n    \"7\": \"B-MISC\",\n    \"8\": \"I-MISC\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"B-LOC\": 5,\n    \"B-MISC\": 7,\n    \"B-ORG\": 3,\n    \"B-PER\": 1,\n    \"I-LOC\": 6,\n    \"I-MISC\": 8,\n    \"I-ORG\": 4,\n    \"I-PER\": 2,\n    \"O\": 0\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28996\n}\n\nloading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\nSome weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(), lr=2e-5)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:17:11.008925Z","iopub.execute_input":"2023-02-08T10:17:11.009816Z","iopub.status.idle":"2023-02-08T10:17:11.019796Z","shell.execute_reply.started":"2023-02-08T10:17:11.009763Z","shell.execute_reply":"2023-02-08T10:17:11.018403Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from accelerate import Accelerator\n\naccelerator = Accelerator()\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:17:11.021658Z","iopub.execute_input":"2023-02-08T10:17:11.022496Z","iopub.status.idle":"2023-02-08T10:17:11.171210Z","shell.execute_reply.started":"2023-02-08T10:17:11.022447Z","shell.execute_reply":"2023-02-08T10:17:11.169707Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"Exception in thread SystemMonitor:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 118, in _start\n    asset.start()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/system/assets/cpu.py\", line 166, in start\n    self.metrics_monitor.start()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 168, in start\n    logger.info(f\"Started {self._process.name}\")\nAttributeError: 'NoneType' object has no attribute 'name'\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import get_scheduler\n\nnum_train_epochs = 3\nnum_update_steps_per_epoch = len(train_dataloader)\nnum_training_steps = num_train_epochs * num_update_steps_per_epoch\n\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:17:11.173002Z","iopub.execute_input":"2023-02-08T10:17:11.177140Z","iopub.status.idle":"2023-02-08T10:17:11.188132Z","shell.execute_reply.started":"2023-02-08T10:17:11.177085Z","shell.execute_reply":"2023-02-08T10:17:11.185673Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import Repository, get_full_repo_name\n\nmodel_name = \"bert-finetuned-ner-accelerate\"\nrepo_name = get_full_repo_name(model_name)\nrepo_name","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:17:11.190122Z","iopub.execute_input":"2023-02-08T10:17:11.191685Z","iopub.status.idle":"2023-02-08T10:17:11.548317Z","shell.execute_reply.started":"2023-02-08T10:17:11.191616Z","shell.execute_reply":"2023-02-08T10:17:11.547170Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"'Arron/bert-finetuned-ner-accelerate'"},"metadata":{}}]},{"cell_type":"code","source":"output_dir = \"bert-finetuned-ner-accelerate\"\nrepo = Repository(output_dir, clone_from=repo_name)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:17:11.549916Z","iopub.execute_input":"2023-02-08T10:17:11.551093Z","iopub.status.idle":"2023-02-08T10:17:13.813525Z","shell.execute_reply.started":"2023-02-08T10:17:11.551048Z","shell.execute_reply":"2023-02-08T10:17:13.811468Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"/kaggle/working/bert-finetuned-ner-accelerate is already a clone of https://huggingface.co/Arron/bert-finetuned-ner-accelerate. Make sure you pull the latest changes with `repo.git_pull()`.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training loop","metadata":{}},{"cell_type":"code","source":"def postprocess(predictions, labels):\n    predictions = predictions.detach().cpu().clone().numpy()\n    labels = labels.detach().cpu().clone().numpy()\n\n    # Remove ignored index (special tokens) and convert to labels\n    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n    true_predictions = [\n        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    return true_labels, true_predictions","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:17:13.820224Z","iopub.execute_input":"2023-02-08T10:17:13.820857Z","iopub.status.idle":"2023-02-08T10:17:13.842429Z","shell.execute_reply.started":"2023-02-08T10:17:13.820802Z","shell.execute_reply":"2023-02-08T10:17:13.841144Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport torch\n\nprogress_bar = tqdm(range(num_training_steps))\n\nfor epoch in range(num_train_epochs):\n    # Training\n    model.train()\n    for batch in train_dataloader:\n        outputs = model(**batch)\n        loss = outputs.loss\n        accelerator.backward(loss)\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n\n    # Evaluation\n    model.eval()\n    for batch in eval_dataloader:\n        with torch.no_grad():\n            outputs = model(**batch)\n\n        predictions = outputs.logits.argmax(dim=-1)\n        labels = batch[\"labels\"]\n\n        # Necessary to pad predictions and labels for being gathered\n        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n\n        predictions_gathered = accelerator.gather(predictions)\n        labels_gathered = accelerator.gather(labels)\n\n        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n        metric.add_batch(predictions=true_predictions, references=true_labels)\n\n    results = metric.compute()\n    print(\n        f\"epoch {epoch}:\",\n        {\n            key: results[f\"overall_{key}\"]\n            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n        },\n    )\n\n    # Save and upload\n    accelerator.wait_for_everyone()\n    unwrapped_model = accelerator.unwrap_model(model)\n    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n    if accelerator.is_main_process:\n        tokenizer.save_pretrained(output_dir)\n        repo.push_to_hub(\n            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n        )","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:17:13.850915Z","iopub.execute_input":"2023-02-08T10:17:13.853776Z","iopub.status.idle":"2023-02-08T10:25:18.529373Z","shell.execute_reply.started":"2023-02-08T10:17:13.853726Z","shell.execute_reply":"2023-02-08T10:25:18.527558Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"Exception in thread SystemMonitor:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 118, in _start\n    asset.start()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/system/assets/cpu.py\", line 166, in start\n    self.metrics_monitor.start()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 168, in start\n    logger.info(f\"Started {self._process.name}\")\nAttributeError: 'NoneType' object has no attribute 'name'\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5268 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92064c4dd6204dd4af5084685b2b51b6"}},"metadata":{}},{"name":"stderr","text":"Configuration saved in bert-finetuned-ner-accelerate/config.json\n","output_type":"stream"},{"name":"stdout","text":"epoch 0: {'precision': 0.9335240659710535, 'recall': 0.9031260175838489, 'f1': 0.9180734856007944, 'accuracy': 0.9830605757343851}\n","output_type":"stream"},{"name":"stderr","text":"Model weights saved in bert-finetuned-ner-accelerate/pytorch_model.bin\ntokenizer config file saved in bert-finetuned-ner-accelerate/tokenizer_config.json\nSpecial tokens file saved in bert-finetuned-ner-accelerate/special_tokens_map.json\nSeveral commits (2) will be pushed upstream.\nConfiguration saved in bert-finetuned-ner-accelerate/config.json\n","output_type":"stream"},{"name":"stdout","text":"epoch 1: {'precision': 0.9432850891955571, 'recall': 0.9197571381686905, 'f1': 0.9313725490196079, 'accuracy': 0.9837228468829105}\n","output_type":"stream"},{"name":"stderr","text":"Model weights saved in bert-finetuned-ner-accelerate/pytorch_model.bin\ntokenizer config file saved in bert-finetuned-ner-accelerate/tokenizer_config.json\nSpecial tokens file saved in bert-finetuned-ner-accelerate/special_tokens_map.json\nConfiguration saved in bert-finetuned-ner-accelerate/config.json\n","output_type":"stream"},{"name":"stdout","text":"epoch 2: {'precision': 0.9476607202961965, 'recall': 0.9263036683665077, 'f1': 0.9368604941352633, 'accuracy': 0.9860187201977983}\n","output_type":"stream"},{"name":"stderr","text":"Model weights saved in bert-finetuned-ner-accelerate/pytorch_model.bin\ntokenizer config file saved in bert-finetuned-ner-accelerate/tokenizer_config.json\nSpecial tokens file saved in bert-finetuned-ner-accelerate/special_tokens_map.json\n","output_type":"stream"}]},{"cell_type":"code","source":"accelerator.wait_for_everyone()\nunwrapped_model = accelerator.unwrap_model(model)\nunwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:25:18.533247Z","iopub.execute_input":"2023-02-08T10:25:18.534361Z","iopub.status.idle":"2023-02-08T10:25:20.034683Z","shell.execute_reply.started":"2023-02-08T10:25:18.534305Z","shell.execute_reply":"2023-02-08T10:25:20.031102Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"Configuration saved in bert-finetuned-ner-accelerate/config.json\nModel weights saved in bert-finetuned-ner-accelerate/pytorch_model.bin\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Using the fine-tuned model","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\n# Replace this with your own checkpoint\nmodel_checkpoint = \"huggingface-course/bert-finetuned-ner\"\ntoken_classifier = pipeline(\n    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n)\ntoken_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")","metadata":{"execution":{"iopub.status.busy":"2023-02-08T10:25:20.050491Z","iopub.execute_input":"2023-02-08T10:25:20.054691Z","iopub.status.idle":"2023-02-08T10:25:50.542832Z","shell.execute_reply.started":"2023-02-08T10:25:20.054625Z","shell.execute_reply":"2023-02-08T10:25:50.538065Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"https://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvmohmmxu\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb15483d60a647eebf0eba184e55f5d6"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/89b5cbe52d0bb00707c92604aa32923af1e03e431c6dc7755afa8e12736b8611.61f3554ca83a9cd2ab0687ff37a7d0f4065bc064c4f297e5d2d42e322e4c5f26\ncreating metadata file for /root/.cache/huggingface/transformers/89b5cbe52d0bb00707c92604aa32923af1e03e431c6dc7755afa8e12736b8611.61f3554ca83a9cd2ab0687ff37a7d0f4065bc064c4f297e5d2d42e322e4c5f26\nloading configuration file https://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/89b5cbe52d0bb00707c92604aa32923af1e03e431c6dc7755afa8e12736b8611.61f3554ca83a9cd2ab0687ff37a7d0f4065bc064c4f297e5d2d42e322e4c5f26\nModel config BertConfig {\n  \"_name_or_path\": \"huggingface-course/bert-finetuned-ner\",\n  \"architectures\": [\n    \"BertForTokenClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"O\",\n    \"1\": \"B-PER\",\n    \"2\": \"I-PER\",\n    \"3\": \"B-ORG\",\n    \"4\": \"I-ORG\",\n    \"5\": \"B-LOC\",\n    \"6\": \"I-LOC\",\n    \"7\": \"B-MISC\",\n    \"8\": \"I-MISC\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"B-LOC\": \"5\",\n    \"B-MISC\": \"7\",\n    \"B-ORG\": \"3\",\n    \"B-PER\": \"1\",\n    \"I-LOC\": \"6\",\n    \"I-MISC\": \"8\",\n    \"I-ORG\": \"4\",\n    \"I-PER\": \"2\",\n    \"O\": \"0\"\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28996\n}\n\n2023-02-08 10:25:20.871682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 10:25:20.874033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 10:25:20.874929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 10:25:20.877877: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-02-08 10:25:20.882428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 10:25:20.883527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 10:25:20.884549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 10:25:20.888489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 10:25:20.889487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 10:25:20.890397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 10:25:20.892699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9991 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\nloading configuration file https://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/89b5cbe52d0bb00707c92604aa32923af1e03e431c6dc7755afa8e12736b8611.61f3554ca83a9cd2ab0687ff37a7d0f4065bc064c4f297e5d2d42e322e4c5f26\nModel config BertConfig {\n  \"_name_or_path\": \"huggingface-course/bert-finetuned-ner\",\n  \"architectures\": [\n    \"BertForTokenClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"O\",\n    \"1\": \"B-PER\",\n    \"2\": \"I-PER\",\n    \"3\": \"B-ORG\",\n    \"4\": \"I-ORG\",\n    \"5\": \"B-LOC\",\n    \"6\": \"I-LOC\",\n    \"7\": \"B-MISC\",\n    \"8\": \"I-MISC\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"B-LOC\": \"5\",\n    \"B-MISC\": \"7\",\n    \"B-ORG\": \"3\",\n    \"B-PER\": \"1\",\n    \"I-LOC\": \"6\",\n    \"I-MISC\": \"8\",\n    \"I-ORG\": \"4\",\n    \"I-PER\": \"2\",\n    \"O\": \"0\"\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28996\n}\n\nhttps://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9wslpbf3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/411M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b90ddb766e04ec79d5b4272295913e3"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/53be13866b5e6a7bf270c198a504ba00a1d6c99765a5425c8d8a6f67474c59db.34b5567d0f0878a5afa5157f6e9b2fe7742287d066ae18b16a646cba224cb46f\ncreating metadata file for /root/.cache/huggingface/transformers/53be13866b5e6a7bf270c198a504ba00a1d6c99765a5425c8d8a6f67474c59db.34b5567d0f0878a5afa5157f6e9b2fe7742287d066ae18b16a646cba224cb46f\nloading weights file https://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/53be13866b5e6a7bf270c198a504ba00a1d6c99765a5425c8d8a6f67474c59db.34b5567d0f0878a5afa5157f6e9b2fe7742287d066ae18b16a646cba224cb46f\nAll model checkpoint weights were used when initializing BertForTokenClassification.\n\nAll the weights of BertForTokenClassification were initialized from the model checkpoint at huggingface-course/bert-finetuned-ner.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\nhttps://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpqw5dr9g5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/320 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"652432bcd06d4972a345370e73b395cf"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/b0df7b2f0fed938ea1c03e3bfce55b08731d98d1eb6ca196178bfeb9203c7507.0bbe47aa0e39b09ed05a95f7d42a27299232ce8e9ef28608e8f8a1cb57a74c0a\ncreating metadata file for /root/.cache/huggingface/transformers/b0df7b2f0fed938ea1c03e3bfce55b08731d98d1eb6ca196178bfeb9203c7507.0bbe47aa0e39b09ed05a95f7d42a27299232ce8e9ef28608e8f8a1cb57a74c0a\nhttps://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmps1h3rivv\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa97d8f74516411d8d547ac1f6cbf93d"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/dbadb9cd77a95bc4e921c9457f6c9f87f9654e89c139503e43f3c6abd4aef018.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\ncreating metadata file for /root/.cache/huggingface/transformers/dbadb9cd77a95bc4e921c9457f6c9f87f9654e89c139503e43f3c6abd4aef018.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\nhttps://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn9nytcfh\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1b8671b1c664f88a3118e9ddc835fad"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/450ab56275366591009a03ebe21bfa2523a89a590ac2e4b569920025b849ecf5.1f9d100b22551a7009fb51f1fadb9158af5db04f4c188aceecaa745a1917c983\ncreating metadata file for /root/.cache/huggingface/transformers/450ab56275366591009a03ebe21bfa2523a89a590ac2e4b569920025b849ecf5.1f9d100b22551a7009fb51f1fadb9158af5db04f4c188aceecaa745a1917c983\nhttps://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmprv_4u3yk\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"996ecb65c1524f308342705a449751f5"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/1f7a04f6385a04a9c60686046244d8daaa06489d154d6523ca28c5d8430c74c0.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\ncreating metadata file for /root/.cache/huggingface/transformers/1f7a04f6385a04a9c60686046244d8daaa06489d154d6523ca28c5d8430c74c0.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\nloading file https://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/dbadb9cd77a95bc4e921c9457f6c9f87f9654e89c139503e43f3c6abd4aef018.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\nloading file https://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/450ab56275366591009a03ebe21bfa2523a89a590ac2e4b569920025b849ecf5.1f9d100b22551a7009fb51f1fadb9158af5db04f4c188aceecaa745a1917c983\nloading file https://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/1f7a04f6385a04a9c60686046244d8daaa06489d154d6523ca28c5d8430c74c0.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\nloading file https://huggingface.co/huggingface-course/bert-finetuned-ner/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/b0df7b2f0fed938ea1c03e3bfce55b08731d98d1eb6ca196178bfeb9203c7507.0bbe47aa0e39b09ed05a95f7d42a27299232ce8e9ef28608e8f8a1cb57a74c0a\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"[{'entity_group': 'PER',\n  'score': 0.9988506,\n  'word': 'Sylvain',\n  'start': 11,\n  'end': 18},\n {'entity_group': 'ORG',\n  'score': 0.9647625,\n  'word': 'Hugging Face',\n  'start': 33,\n  'end': 45},\n {'entity_group': 'LOC',\n  'score': 0.9986118,\n  'word': 'Brooklyn',\n  'start': 49,\n  'end': 57}]"},"metadata":{}}]}]}