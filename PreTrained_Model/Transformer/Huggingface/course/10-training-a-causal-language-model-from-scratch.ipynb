{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install transformers[sentencepiece]\n!pip install datasets\n!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2023-02-09T06:42:23.025973Z","iopub.execute_input":"2023-02-09T06:42:23.026798Z","iopub.status.idle":"2023-02-09T06:43:01.562420Z","shell.execute_reply.started":"2023-02-09T06:42:23.026698Z","shell.execute_reply":"2023-02-09T06:43:01.561181Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: transformers[sentencepiece] in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (3.7.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (2.28.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (4.64.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (6.0.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.12.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.10.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (1.21.6)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (23.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (2021.11.10)\nRequirement already satisfied: protobuf<=3.20.1 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (3.19.4)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.1.97)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]) (4.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers[sentencepiece]) (3.8.0)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (1.26.14)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (6.0.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.64.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (23.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2023.1.0)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.10.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (5.0.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.0)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: evaluate in /opt/conda/lib/python3.7/site-packages (0.4.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.10.1)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2023.1.0)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.3.6)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.70.14)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from evaluate) (3.2.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.21.6)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.28.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from evaluate) (23.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from evaluate) (6.0.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.3.5)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.64.0)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (5.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (3.8.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.7.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.1.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2.1.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->evaluate) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2022.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.13.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.4.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"**we will build a scaled-down version of a code generation model**","metadata":{}},{"cell_type":"markdown","source":"# Gathering the data","metadata":{}},{"cell_type":"markdown","source":"This was the approach taken in the Transformers textbook to pretrain a large GPT-2 model. Using a GitHub dump of about 180 GB containing roughly 20 million Python files called codeparrot, the authors built a dataset that they then shared on the Hugging Face Hub.  \nHowever, training on the full corpus is time- and compute-consuming, and we only need the subset of the dataset concerned with the Python data science stack. So, let’s start by filtering the codeparrot dataset for all files that include any of the libraries in this stack. Because of the dataset’s size, we want to avoid downloading it; instead, we’ll use the streaming feature to filter it on the fly. To help us filter the code samples using the libraries we mentioned earlier, we’ll use the following function:","metadata":{}},{"cell_type":"code","source":"def any_keyword_in_string(string, keywords):\n    for keyword in keywords:\n        if keyword in string:\n            return True\n    return False","metadata":{"execution":{"iopub.status.busy":"2023-02-09T06:43:01.567165Z","iopub.execute_input":"2023-02-09T06:43:01.567528Z","iopub.status.idle":"2023-02-09T06:43:01.572732Z","shell.execute_reply.started":"2023-02-09T06:43:01.567495Z","shell.execute_reply":"2023-02-09T06:43:01.571768Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Let’s test it on two examples:\nfilters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\nexample_1 = \"import numpy as np\"\nexample_2 = \"import pandas as pd\"\n\nprint(\n    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-09T06:43:01.573762Z","iopub.execute_input":"2023-02-09T06:43:01.576374Z","iopub.status.idle":"2023-02-09T06:43:01.589075Z","shell.execute_reply.started":"2023-02-09T06:43:01.576335Z","shell.execute_reply":"2023-02-09T06:43:01.587983Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"False True\n","output_type":"stream"}]},{"cell_type":"code","source":"# We can use this to create a function that will stream the dataset and filter the elements we want:\nfrom collections import defaultdict\nfrom tqdm import tqdm\nfrom datasets import Dataset\n\n\ndef filter_streaming_dataset(dataset, filters):\n    filtered_dict = defaultdict(list)\n    total = 0\n    for sample in tqdm(iter(dataset)):\n        total += 1\n        if any_keyword_in_string(sample[\"content\"], filters):\n            for k, v in sample.items():\n                filtered_dict[k].append(v)\n    print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n    return Dataset.from_dict(filtered_dict)","metadata":{"execution":{"iopub.status.busy":"2023-02-09T06:43:01.592059Z","iopub.execute_input":"2023-02-09T06:43:01.592468Z","iopub.status.idle":"2023-02-09T06:43:02.375236Z","shell.execute_reply.started":"2023-02-09T06:43:01.592431Z","shell.execute_reply":"2023-02-09T06:43:02.374270Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# This cell will take a very long time to execute, so you should skip it and go to\n# the next one!\n# from datasets import load_dataset\n\n# split = \"train\"  # \"valid\"\n# filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n\n# data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n# filtered_data = filter_streaming_dataset(data, filters)","metadata":{"execution":{"iopub.status.busy":"2023-02-09T06:43:02.376767Z","iopub.execute_input":"2023-02-09T06:43:02.377438Z","iopub.status.idle":"2023-02-09T06:43:02.383786Z","shell.execute_reply.started":"2023-02-09T06:43:02.377397Z","shell.execute_reply":"2023-02-09T06:43:02.382811Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Filtering the full dataset can take 2-3h depending on your machine and bandwidth. If you don’t want to go through this lengthy process yourself, we provide the filtered dataset on the Hub for you to download:","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\n\nds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\nds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n\nraw_datasets = DatasetDict(\n    {\n        \"train\": ds_train,  # .shuffle().select(range(50000)),\n        \"valid\": ds_valid,  # .shuffle().select(range(500))\n    }\n)\n\nraw_datasets","metadata":{"execution":{"iopub.status.busy":"2023-02-09T06:43:02.385255Z","iopub.execute_input":"2023-02-09T06:43:02.385715Z","iopub.status.idle":"2023-02-09T06:43:32.068271Z","shell.execute_reply.started":"2023-02-09T06:43:02.385672Z","shell.execute_reply":"2023-02-09T06:43:32.067153Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n        num_rows: 606720\n    })\n    valid: Dataset({\n        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n        num_rows: 3322\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Let’s look at an example from the dataset. \nfor key in raw_datasets[\"train\"][0]:\n    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")","metadata":{"execution":{"iopub.status.busy":"2023-02-09T06:43:32.069717Z","iopub.execute_input":"2023-02-09T06:43:32.070629Z","iopub.status.idle":"2023-02-09T06:43:32.079558Z","shell.execute_reply.started":"2023-02-09T06:43:32.070586Z","shell.execute_reply":"2023-02-09T06:43:32.078393Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"REPO_NAME: kmike/scikit-learn\nPATH: sklearn/utils/__init__.py\nCOPIES: 3\nSIZE: 10094\nCONTENT: \"\"\"\nThe :mod:`sklearn.utils` module includes various utilites.\n\"\"\"\n\nfrom collections import Sequence\n\nimport numpy as np\nfrom scipy.sparse import issparse\nimport warnings\n\nfrom .murmurhash import murm\nLICENSE: bsd-3-clause\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Most documents contain many more than 128 tokens, so simply truncating the inputs to the maximum length would eliminate a large fraction of our dataset. Instead, we’ll use the return_overflowing_tokens option to tokenize the whole input and split it into several chunks, as we did in Chapter 6. We’ll also use the return_length option to return the length of each created chunk automatically. Often the last chunk will be smaller than the context size, and we’ll get rid of these pieces to avoid padding issues; we don’t really need them as we have plenty of data anyway.","metadata":{}},{"cell_type":"code","source":"# Let’s see exactly how this works by looking at the first two examples:\nfrom transformers import AutoTokenizer\n\ncontext_length = 128\ntokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n\noutputs = tokenizer(\n    raw_datasets[\"train\"][:2][\"content\"],\n    truncation=True,\n    max_length=context_length,\n    return_overflowing_tokens=True,\n    return_length=True,\n)\n\nprint(f\"Input IDs length: {len(outputs['input_ids'])}\")\nprint(f\"Input chunk lengths: {(outputs['length'])}\")\nprint(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")\n# With the overflow_to_sample_mapping field, we can also reconstruct which chunks belonged to which input samples.","metadata":{"execution":{"iopub.status.busy":"2023-02-09T06:43:32.081592Z","iopub.execute_input":"2023-02-09T06:43:32.082473Z","iopub.status.idle":"2023-02-09T06:43:35.138456Z","shell.execute_reply.started":"2023-02-09T06:43:32.082434Z","shell.execute_reply":"2023-02-09T06:43:35.137282Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Input IDs length: 34\nInput chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]\nChunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize(element):\n    outputs = tokenizer(\n        element[\"content\"],\n        truncation=True,\n        max_length=context_length,\n        return_overflowing_tokens=True,\n        return_length=True,\n    )\n    input_batch = []\n    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n        if length == context_length:\n            input_batch.append(input_ids)\n    return {\"input_ids\": input_batch}\n\n\ntokenized_datasets = raw_datasets.map(\n    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n)\ntokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2023-02-09T06:43:35.141549Z","iopub.execute_input":"2023-02-09T06:43:35.141854Z","iopub.status.idle":"2023-02-09T08:19:16.497336Z","shell.execute_reply.started":"2023-02-09T06:43:35.141823Z","shell.execute_reply":"2023-02-09T08:19:16.496191Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/607 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7ea5827352d43989fe76cb9525d918d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99de87fa07744d0b8b22f94f5139c704"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids'],\n        num_rows: 16702061\n    })\n    valid: Dataset({\n        features: ['input_ids'],\n        num_rows: 93164\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"OpenAI’s GPT-3 and Codex models are trained on 300 and 100 billion tokens, respectively, where the Codex models are initialized from the GPT-3 checkpoints. Our goal in this section is not to compete with these models, which can generate long, coherent texts, but to create a scaled-down version providing a quick autocomplete function for data scientists.","metadata":{}},{"cell_type":"markdown","source":"# Initializing a new model","metadata":{}},{"cell_type":"code","source":"# Our first step is to freshly initialize a GPT-2 model. We’ll use the same configuration for our model as for the small GPT-2 model\nfrom transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n\nconfig = AutoConfig.from_pretrained(\n    \"gpt2\",\n    vocab_size=len(tokenizer),\n    n_ctx=context_length,\n    bos_token_id=tokenizer.bos_token_id,\n    eos_token_id=tokenizer.eos_token_id,\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:16.501625Z","iopub.execute_input":"2023-02-09T08:19:16.501929Z","iopub.status.idle":"2023-02-09T08:19:19.562425Z","shell.execute_reply.started":"2023-02-09T08:19:16.501901Z","shell.execute_reply":"2023-02-09T08:19:19.561290Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"# With that configuration, we can load a new model. Note that this is the first time we don’t use the from_pretrained() function, since we’re actually initializing a model ourself:\nmodel = GPT2LMHeadModel(config)\nmodel_size = sum(t.numel() for t in model.parameters())\nprint(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:19.567284Z","iopub.execute_input":"2023-02-09T08:19:19.569914Z","iopub.status.idle":"2023-02-09T08:19:25.182608Z","shell.execute_reply.started":"2023-02-09T08:19:19.569868Z","shell.execute_reply":"2023-02-09T08:19:25.181490Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"GPT-2 size: 124.2M parameters\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can use the DataCollatorForLanguageModeling collator, which is designed specifically for language modeling (as the name subtly suggests). Besides stacking and padding batches, it also takes care of creating the language model labels — in causal language modeling the inputs serve as labels too (just shifted by one element), and this data collator creates them on the fly during training so we don’t need to duplicate the input_ids.","metadata":{}},{"cell_type":"markdown","source":"Note that **DataCollatorForLanguageModeling supports both masked language modeling (MLM) and causal language modeling (CLM)**. By default it prepares data for MLM, but we can switch to CLM by setting the argument mlm=False:","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\ntokenizer.pad_token = tokenizer.eos_token\ndata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:25.184359Z","iopub.execute_input":"2023-02-09T08:19:25.184730Z","iopub.status.idle":"2023-02-09T08:19:29.667201Z","shell.execute_reply.started":"2023-02-09T08:19:25.184691Z","shell.execute_reply":"2023-02-09T08:19:29.666122Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Let’s have a look at an example:\nout = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\nfor key in out:\n    print(f\"{key} shape: {out[key].shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:29.668764Z","iopub.execute_input":"2023-02-09T08:19:29.669128Z","iopub.status.idle":"2023-02-09T08:19:29.677605Z","shell.execute_reply.started":"2023-02-09T08:19:29.669087Z","shell.execute_reply":"2023-02-09T08:19:29.676281Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"input_ids shape: torch.Size([5, 128])\nattention_mask shape: torch.Size([5, 128])\nlabels shape: torch.Size([5, 128])\n","output_type":"stream"}]},{"cell_type":"code","source":"# for jupyter\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n\n# for terminal\n# huggingface-cli login","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:29.679060Z","iopub.execute_input":"2023-02-09T08:19:29.679698Z","iopub.status.idle":"2023-02-09T08:19:29.727671Z","shell.execute_reply.started":"2023-02-09T08:19:29.679645Z","shell.execute_reply":"2023-02-09T08:19:29.726647Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75a5bd5bb88b4b1587ffc8be449e64a4"}},"metadata":{}}]},{"cell_type":"markdown","source":"We’ll use a cosine learning rate schedule with some warmup and an effective batch size of 256 (per_device_train_batch_size * gradient_accumulation_steps). Gradient accumulation is used when a single batch does not fit into memory, and incrementally builds up the gradient through several forward/backward passes. We’ll see this in action when we create the training loop with 🤗 Accelerate.","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"codeparrot-ds\",\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    evaluation_strategy=\"steps\",\n    eval_steps=5_000,\n    logging_steps=5_000,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    weight_decay=0.1,\n    warmup_steps=1_000,\n    lr_scheduler_type=\"cosine\",\n    learning_rate=5e-4,\n    save_steps=5_000,\n    fp16=True,\n    push_to_hub=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=args,\n    data_collator=data_collator,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"valid\"],\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:29.729443Z","iopub.execute_input":"2023-02-09T08:19:29.730112Z","iopub.status.idle":"2023-02-09T08:19:39.983123Z","shell.execute_reply.started":"2023-02-09T08:19:29.730073Z","shell.execute_reply":"2023-02-09T08:19:39.982032Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/codeparrot-ds is already a clone of https://huggingface.co/Arron/codeparrot-ds. Make sure you pull the latest changes with `repo.git_pull()`.\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"Using cuda_amp half precision backend\n","output_type":"stream"}]},{"cell_type":"code","source":"#trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:39.985322Z","iopub.execute_input":"2023-02-09T08:19:39.986605Z","iopub.status.idle":"2023-02-09T08:19:39.992954Z","shell.execute_reply.started":"2023-02-09T08:19:39.986556Z","shell.execute_reply":"2023-02-09T08:19:39.991924Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# After training completes, we can push the model and tokenizer to the Hub:\n# trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:39.994593Z","iopub.execute_input":"2023-02-09T08:19:39.994953Z","iopub.status.idle":"2023-02-09T08:19:40.006726Z","shell.execute_reply.started":"2023-02-09T08:19:39.994916Z","shell.execute_reply":"2023-02-09T08:19:40.005501Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Code generation with a pipeline","metadata":{}},{"cell_type":"code","source":"# import torch\n# from transformers import pipeline\n\n# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n# pipe = pipeline(\n#     \"text-generation\", model=\"huggingface-course/codeparrot-ds\", device=device\n# )","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:40.008185Z","iopub.execute_input":"2023-02-09T08:19:40.008660Z","iopub.status.idle":"2023-02-09T08:19:40.019073Z","shell.execute_reply.started":"2023-02-09T08:19:40.008618Z","shell.execute_reply":"2023-02-09T08:19:40.017906Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# txt = \"\"\"\\\n# # create some data\n# x = np.random.randn(100)\n# y = np.random.randn(100)\n\n# # create scatter plot with x, y\n# \"\"\"\n# print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:40.020896Z","iopub.execute_input":"2023-02-09T08:19:40.021645Z","iopub.status.idle":"2023-02-09T08:19:40.031598Z","shell.execute_reply.started":"2023-02-09T08:19:40.021605Z","shell.execute_reply":"2023-02-09T08:19:40.030492Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Training with 🤗 Accelerate","metadata":{}},{"cell_type":"markdown","source":"We’ve seen how to train a model with the Trainer, which can allow for some customization. However, sometimes we want full control over the training loop, or we want to make some exotic changes. In this case 🤗 Accelerate is a great choice, and in this section we’ll go through the steps to use it to train our model. To make things more interesting, we’ll also add a twist to the training loop.","metadata":{}},{"cell_type":"code","source":"keytoken_ids = []\nfor keyword in [\n    \"plt\",\n    \"pd\",\n    \"sk\",\n    \"fit\",\n    \"predict\",\n    \" plt\",\n    \" pd\",\n    \" sk\",\n    \" fit\",\n    \" predict\",\n    \"testtest\",\n]:\n    ids = tokenizer([keyword]).input_ids[0]\n    if len(ids) == 1:\n        keytoken_ids.append(ids[0])\n    else:\n        print(f\"Keyword has not single token: {keyword}\")","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:40.033377Z","iopub.execute_input":"2023-02-09T08:19:40.034106Z","iopub.status.idle":"2023-02-09T08:19:40.046918Z","shell.execute_reply.started":"2023-02-09T08:19:40.034068Z","shell.execute_reply":"2023-02-09T08:19:40.045868Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Keyword has not single token: testtest\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.nn import CrossEntropyLoss\nimport torch\n\n\ndef keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n    # Shift so that tokens < n predict n\n    shift_labels = inputs[..., 1:].contiguous()\n    shift_logits = logits[..., :-1, :].contiguous()\n    # Calculate per-token loss\n    loss_fct = CrossEntropyLoss(reduce=False)\n    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n    # Resize and average loss per sample\n    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n    # Calculate and scale weighting\n    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(\n        axis=[0, 2]\n    )\n    weights = alpha * (1.0 + weights)\n    # Calculate weighted average\n    weighted_loss = (loss_per_sample * weights).mean()\n    return weighted_loss","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:40.048899Z","iopub.execute_input":"2023-02-09T08:19:40.049725Z","iopub.status.idle":"2023-02-09T08:19:40.059123Z","shell.execute_reply.started":"2023-02-09T08:19:40.049682Z","shell.execute_reply":"2023-02-09T08:19:40.057875Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data.dataloader import DataLoader\n\ntokenized_datasets.set_format(\"torch\")\ntrain_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=32, shuffle=True)\neval_dataloader = DataLoader(tokenized_datasets[\"valid\"], batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:40.060870Z","iopub.execute_input":"2023-02-09T08:19:40.061607Z","iopub.status.idle":"2023-02-09T08:19:40.076919Z","shell.execute_reply.started":"2023-02-09T08:19:40.061569Z","shell.execute_reply":"2023-02-09T08:19:40.075847Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"weight_decay = 0.1\n\n\ndef get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n    params_with_wd, params_without_wd = [], []\n    for n, p in model.named_parameters():\n        if any(nd in n for nd in no_decay):\n            params_without_wd.append(p)\n        else:\n            params_with_wd.append(p)\n    return [\n        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n    ]","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:40.078743Z","iopub.execute_input":"2023-02-09T08:19:40.079503Z","iopub.status.idle":"2023-02-09T08:19:40.089778Z","shell.execute_reply.started":"2023-02-09T08:19:40.079462Z","shell.execute_reply":"2023-02-09T08:19:40.088693Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def evaluate():\n    model.eval()\n    losses = []\n    for step, batch in enumerate(eval_dataloader):\n        with torch.no_grad():\n            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n\n        losses.append(accelerator.gather(outputs.loss))\n    loss = torch.mean(torch.cat([torch.Tensor(losses)]))\n    try:\n        perplexity = torch.exp(loss)\n    except OverflowError:\n        perplexity = float(\"inf\")\n    return loss.item(), perplexity.item()","metadata":{"execution":{"iopub.status.busy":"2023-02-09T10:13:03.954435Z","iopub.execute_input":"2023-02-09T10:13:03.955323Z","iopub.status.idle":"2023-02-09T10:13:03.962571Z","shell.execute_reply.started":"2023-02-09T10:13:03.955280Z","shell.execute_reply":"2023-02-09T10:13:03.961388Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# With the evaluate() function we can report loss and perplexity at regular intervals. Next, we redefine our model to make sure we train from scratch again:\nmodel = GPT2LMHeadModel(config)","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:40.105614Z","iopub.execute_input":"2023-02-09T08:19:40.106370Z","iopub.status.idle":"2023-02-09T08:19:45.505171Z","shell.execute_reply.started":"2023-02-09T08:19:40.106330Z","shell.execute_reply":"2023-02-09T08:19:45.504102Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from torch.optim import AdamW\n\noptimizer = AdamW(get_grouped_params(model), lr=5e-4)","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:45.506534Z","iopub.execute_input":"2023-02-09T08:19:45.506947Z","iopub.status.idle":"2023-02-09T08:19:45.512731Z","shell.execute_reply.started":"2023-02-09T08:19:45.506900Z","shell.execute_reply":"2023-02-09T08:19:45.511681Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from accelerate import Accelerator\n\naccelerator = Accelerator(fp16=True)\n\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:45.514417Z","iopub.execute_input":"2023-02-09T08:19:45.515206Z","iopub.status.idle":"2023-02-09T08:19:45.673054Z","shell.execute_reply.started":"2023-02-09T08:19:45.515167Z","shell.execute_reply":"2023-02-09T08:19:45.672065Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from transformers import get_scheduler\n\nnum_train_epochs = 1\nnum_update_steps_per_epoch = len(train_dataloader)\nnum_training_steps = num_train_epochs * num_update_steps_per_epoch\n\nlr_scheduler = get_scheduler(\n    name=\"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=1_000,\n    num_training_steps=num_training_steps,\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:45.678621Z","iopub.execute_input":"2023-02-09T08:19:45.678920Z","iopub.status.idle":"2023-02-09T08:19:45.684602Z","shell.execute_reply.started":"2023-02-09T08:19:45.678892Z","shell.execute_reply":"2023-02-09T08:19:45.683431Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import Repository, get_full_repo_name\n\nmodel_name = \"codeparrot-ds-accelerate\"\nrepo_name = get_full_repo_name(model_name)\nrepo_name","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:45.686127Z","iopub.execute_input":"2023-02-09T08:19:45.686817Z","iopub.status.idle":"2023-02-09T08:19:46.049192Z","shell.execute_reply.started":"2023-02-09T08:19:45.686778Z","shell.execute_reply":"2023-02-09T08:19:46.048122Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"'Arron/codeparrot-ds-accelerate'"},"metadata":{}}]},{"cell_type":"code","source":"output_dir = \"codeparrot-ds-accelerate\"\nrepo = Repository(output_dir, clone_from=repo_name)","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:19:46.050822Z","iopub.execute_input":"2023-02-09T08:19:46.051251Z","iopub.status.idle":"2023-02-09T08:19:48.293201Z","shell.execute_reply.started":"2023-02-09T08:19:46.051211Z","shell.execute_reply":"2023-02-09T08:19:48.291877Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/codeparrot-ds-accelerate is already a clone of https://huggingface.co/Arron/codeparrot-ds-accelerate. Make sure you pull the latest changes with `repo.git_pull()`.\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"evaluate()","metadata":{"execution":{"iopub.status.busy":"2023-02-09T10:13:10.431402Z","iopub.execute_input":"2023-02-09T10:13:10.432192Z","iopub.status.idle":"2023-02-09T10:22:47.776990Z","shell.execute_reply.started":"2023-02-09T10:13:10.432124Z","shell.execute_reply":"2023-02-09T10:22:47.775526Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"(11.015778541564941, 60826.359375)"},"metadata":{}}]},{"cell_type":"markdown","source":"Those are very high values for loss and perplexity, but that’s not surprising as we haven’t trained the model yet. With that, we have everything prepared to write the core part of the training script: the training loop. In the training loop we iterate over the dataloader and pass the batches to the model. With the logits, we can then evaluate our custom loss function. We scale the loss by the number of gradient accumulation steps so as not to create larger losses when aggregating more steps. Before we optimize, we also clip the gradients for better convergence. Finally, every few steps we evaluate the model on the evaluation set with our new evaluate() function:","metadata":{}},{"cell_type":"code","source":"from tqdm.notebook import tqdm\n\ngradient_accumulation_steps = 8\neval_steps = 5_000\n\nmodel.train()\ncompleted_steps = 0\nfor epoch in range(num_train_epochs):\n    for step, batch in tqdm(\n        enumerate(train_dataloader, start=1), total=num_training_steps\n    ):\n        logits = model(batch[\"input_ids\"]).logits\n        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n        if step % 100 == 0:\n            accelerator.print(\n                {\n                    \"lr\": lr_scheduler,\n                    \"samples\": step * samples_per_step,\n                    \"steps\": completed_steps,\n                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n                }\n            )\n        loss = loss / gradient_accumulation_steps\n        accelerator.backward(loss)\n        if step % gradient_accumulation_steps == 0:\n            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n            completed_steps += 1\n        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n            eval_loss, perplexity = evaluate()\n            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n            model.train()\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(output_dir)\n                repo.push_to_hub(\n                    commit_message=f\"Training in progress step {step}\", blocking=False\n                )","metadata":{"execution":{"iopub.status.busy":"2023-02-09T11:16:04.975565Z","iopub.execute_input":"2023-02-09T11:16:04.975994Z","iopub.status.idle":"2023-02-09T11:16:04.984793Z","shell.execute_reply.started":"2023-02-09T11:16:04.975957Z","shell.execute_reply":"2023-02-09T11:16:04.983711Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}