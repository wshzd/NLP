全词mask的bert
https://github.com/ymcui/Chinese-BERT-wwm

bert源码解析
http://fancyerii.github.io/2019/03/09/bert-codes/

pytorch1.0.0和tensorflow2.0.0-rc1支持的预训练模型
https://github.com/huggingface/transformers

DistilBERT的应用
https://github.com/drlego9/transformers-examples/tree/master/distillation

bert on OCR
https://github.com/tiantian91091317/OCR-Corrector

性能媲美BERT，但参数量仅为1/300，这是谷歌最新的NLP模型--PRADO 
https://www.aclweb.org/anthology/D19-1506.pdf
改进PRADO的模型pQRNN 
https://github.com/tensorflow/models/tree/master/research/sequence_projection

