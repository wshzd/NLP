# NLP PTM 
## 生成式预训练模型

## MLM预训练模型

## 多任务预训练模型

## 多模预训练模型

## 通用预训练模型

## 预训练模型开源工具

# Multi-modal PTM
## VLP PTM
[《Vision-Language Intelligence: Tasks, Representation Learning, and Large Models》](arXiv:2203.01922.pdf)



基于keras的轻量级bert
https://github.com/bojone/bert4keras

ALbert模型的实现
https://link.zhihu.com/?target=https%3A//github.com/brightmart/albert_zh

使用PaddlePaddle做词向量skip-gram
http://www.360doc.com/content/19/0816/04/46368139_855209381.shtml

bert for tf2.0
https://github.com/kpe/bert-for-tf2

adapter-bert的地址
https://github.com/google-research/adapter-bert/

bert作为服务应用
https://github.com/hanxiao/bert-as-service

Skip-Thought Vectors
https://arxiv.org/pdf/1506.06726.pdf

BioBERT: 用于生物医学文本挖掘的预先训练生物医学语言表示模型
fine-tuning代码
https://github.com/dmis-lab/biobert
BioBERT预训练模型权重下载
https://github.com/naver/biobert-pretrained
BioBERT论文解读
https://blog.csdn.net/devshilei/article/details/103887864

Bert可视化工具bertviz
https://github.com/jessevig/bertviz
https://blog.csdn.net/duan_zhihua/article/details/87388646
https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1

BERT论文合集
https://github.com/tomohideshibata/BERT-related-papers#domain-specific

全面剖析Transformer
https://jalammar.github.io/illustrated-transformer/

Transformer
https://arxiv.org/abs/1706.03762.pdf
tensorflow版本：https://github.com/tensorflow/tensor2tensor
pytorch版本：http://nlp.seas.harvard.edu/2018/04/03/attention.html

Transformer-XL
https://arxiv.org/pdf/1901.02860.pdf
https://github.com/kimiyoung/transformer-xl
https://www.lyrn.ai/2019/01/16/transformer-xl-sota-language-model/
https://blog.csdn.net/magical_bubble/article/details/89060213

Kashgari代码成果
https://kashgari.readthedocs.io/en/v1.1.1/embeddings/bert-embedding_v2.html

基于对抗学习的ELECTRA_TINY预训练模型
https://github.com/CLUEbenchmark/ELECTRA
https://github.com/google-research/electra

Transformers大集合
https://huggingface.co/transformers/index.html
https://github.com/huggingface/transformers

可视化BERTs预训练模型
https://github.com/jessevig/bertviz

知识蒸馏
https://github.com/airaria/TextBrewer

追一科技
https://github.com/ZhuiyiTechnology/simbert

GPT3预训练模型
https://github.com/openai/gpt-3

ACL 2020 的论文 《Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning》对BERT的MLM机制低效问题进行了改进
https://arxiv.org/abs/2004.08097 
https://github.com/joongbo/tta

基于bert的ocr识别结果校正
https://github.com/tiantian91091317/OCR-Corrector

性能媲美BERT，但参数量仅为1/300，这是谷歌最新的NLP模型--PRADO 
https://www.aclweb.org/anthology/D19-1506.pdf
改进PRADO的模型pQRNN 
https://github.com/tensorflow/models/tree/master/research/sequence_projection

神州泰岳AI研究院与中科院深圳先进技术研究院合作推出BERT-EMD：借助EMD实现多对多层映射的BERT压缩方法
论文：https://arxiv.org/abs/2010.06133
代码：https://github.com/lxk00/BERT-EMD
介绍：https://hub.baai.ac.cn/view/2768

2020年3月，哈工大讯飞联合实验室推出了中文ELECTRA预训练模型，ELECTRA提出了一套新的预训练框架，其中包含两个部分：Generator和Discriminator
http://github.com/ymcui/Chinese-ELECTRA

清华和阿里的paper：使用bert解决长文本
《CogLTX: Applying BERT to Long Texts》
论文链接：
http://keg.cs.tsinghua.edu.cn/jietang/publications/NIPS20-Ding-et-al-CogLTX.pdf
Github：
https://github.com/Sleepychord/CogLTX

《Adversarial Training for Large Neural Language Models》首次全面对「对抗预训练」进行了全面研究，建立了一种统一的、适配于语言模型的对抗训练框架——ALUM (Adversarial training for large neural LangUage Models)
论文下载地址：https://arxiv.org/pdf/2004.08994
论文开源地址：https://github.com/namisan/mt-dnn

《Rethinking Attention with Performers》---对Transformer中的self-Attention的改进
论文地址:https://www.aminer.cn/pub/5f75feb191e0111c1eb4dbb2/rethinking-attention-with-performers
论文代码：https://github.com/google-research/google-research/tree/master/performer

《ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning》实体和关系抽取的预训练模型
论文链接：https://arxiv.org/abs/2012.15022
开源链接：https://github.com/thunlp/ERICA

《2109Paradigm Shift in Natural Language Processing》
论文链接：https://arxiv.org/abs/2109.12575
项目网站：https://txsun1997.github.io/nlp-paradigm-shift/
Slides：https://txsun1997.github.io/slides/nlp-paradigm-shift.pdf

2021.09微软和英伟达联合发布的MT-NLG预训练模型《Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model》
https://github.com/NVIDIA/Megatron-LM
英伟达官网：https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/

微软NLG预训练模型：《BANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining》
论文地址：https://arxiv.org/abs/2012.15525
github地址：https://github.com/microsoft/BANG

《Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese》
论文链接：https://arxiv.org/pdf/2110.06696.pdf 
论文代码：https://github.com/Langboat/Mengzi

《DeepNet_ Scaling Transformers to 1,000 Layers》
