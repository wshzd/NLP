# NLP PTM 
## Survey
2202[A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models](https://arxiv.org/abs/2202.08772.pdf) by Da Yin, Li Dong, Hao Cheng, Xiaodong Liu, Kai-Wei Chang, Furu Wei, Jianfeng Gao

2202[A Survey of Pretraining on Graphs: Taxonomy, Methods, and Applications](https://arxiv.org/abs/2202.07893.pdf) by Jun Xia, Yanqiao Zhu, Yuanqi Du, Stan Z. Li

2202[Threats to Pre-trained Language Models: Survey and Taxonomy](https://arxiv.org/abs/2202.06862.pdf) by Shangwei Guo, Chunlong Xie, Jiwei Li, Lingjuan Lyu, Tianwei Zhang

2201[Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey](https://arxiv.org/abs/2201.12438.pdf) by Prajjwal Bhargava, Vincent Ng

2201[Table Pre-training: A Survey on Model Architectures, Pretraining Objectives, and Downstream Tasks](https://arxiv.org/abs/2201.09745.pdf) by Haoyu Dong, Zhoujun Cheng, Xinyi He, Mengyu Zhou, Anda Zhou, Fan Zhou, Ao Liu, Shi Han, Dongmei Zhang

2201[A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models](https://arxiv.org/abs/2201.05337.pdf) by Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, Dawei Song

2111[A Survey of Large-Scale Deep Learning Serving System Optimization: Challenges and Opportunities](https://arxiv.org/abs/2111.14247.pdf) by Fuxun Yu, Di Wang, Longfei Shangguan, Minjia Zhang, Xulong Tang, Chenchen Liu, Xiang Chen

2111[Pre-training Methods in Information Retrieval](https://arxiv.org/abs/2111.13853.pdf) by Yixing Fan, Xiaohui Xie, Yinqiong Cai, Jia Chen, Xinyu Ma, Xiangsheng Li, Ruqing Zhang, Jiafeng Guo, Yiqun Liu

2111[Temporal Effects on Pre-trained Models for Language Processing Tasks](https://arxiv.org/abs/2111.12790.pdf) by Oshin Agarwal, Ani Nenkova

2111[Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey](https://arxiv.org/abs/2111.01243.pdf) by Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heinz, Dan Roth

2110[Småprat: DialoGPT for Natural Language Generation of Swedish Dialogue by Transfer Learning](https://arxiv.org/abs/2110.06273.pdf) by Tosin Adewumi, Rickard Brännvall, Nosheen Abid, Maryam Pahlavan, Sana Sabah Sabry, Foteini Liwicki, Marcus Liwicki

2110[Pre-trained Language Models in Biomedical Domain: A Systematic Survey](https://arxiv.org/abs/2110.05006.pdf) by Benyou Wang, Qianqian Xie, Jiahuan Pei, Prayag Tiwari, Zhao Li, Jie fu

2110[A Survey of Knowledge Enhanced Pre-trained Models](https://arxiv.org/abs/2110.00269.pdf) by Jian Yang, Gang Xiao, Yulong Shen, Wei Jiang, Xinyu Hu, Ying Zhang, Jinghui Peng

2109[On the Universality of Deep Contextual Language Models](https://arxiv.org/abs/2109.07140.pdf) by Shaily Bhatt, Poonam Goyal, Sandipan Dandapat, Monojit Choudhury, Sunayana Sitaram



### Federated Learning
2107[Federated Learning Meets Natural Language Processing: A Survey](https://arxiv.org/abs/2107.12603 .pdf) by Ming Liu, Stella Ho, Mengqi Wang, Longxiang Gao, Yuan Jin, He Zhang














基于keras的轻量级bert
https://github.com/bojone/bert4keras

ALbert模型的实现
https://link.zhihu.com/?target=https%3A//github.com/brightmart/albert_zh

使用PaddlePaddle做词向量skip-gram
http://www.360doc.com/content/19/0816/04/46368139_855209381.shtml

bert for tf2.0
https://github.com/kpe/bert-for-tf2

adapter-bert的地址
https://github.com/google-research/adapter-bert/

bert作为服务应用
https://github.com/hanxiao/bert-as-service

Skip-Thought Vectors
https://arxiv.org/pdf/1506.06726.pdf

BioBERT: 用于生物医学文本挖掘的预先训练生物医学语言表示模型
fine-tuning代码
https://github.com/dmis-lab/biobert
BioBERT预训练模型权重下载
https://github.com/naver/biobert-pretrained
BioBERT论文解读
https://blog.csdn.net/devshilei/article/details/103887864

Bert可视化工具bertviz
https://github.com/jessevig/bertviz
https://blog.csdn.net/duan_zhihua/article/details/87388646
https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1

BERT论文合集
https://github.com/tomohideshibata/BERT-related-papers#domain-specific

全面剖析Transformer
https://jalammar.github.io/illustrated-transformer/

Transformer
https://arxiv.org/abs/1706.03762.pdf
tensorflow版本：https://github.com/tensorflow/tensor2tensor
pytorch版本：http://nlp.seas.harvard.edu/2018/04/03/attention.html

Transformer-XL
https://arxiv.org/pdf/1901.02860.pdf
https://github.com/kimiyoung/transformer-xl
https://www.lyrn.ai/2019/01/16/transformer-xl-sota-language-model/
https://blog.csdn.net/magical_bubble/article/details/89060213

Kashgari代码成果
https://kashgari.readthedocs.io/en/v1.1.1/embeddings/bert-embedding_v2.html

基于对抗学习的ELECTRA_TINY预训练模型
https://github.com/CLUEbenchmark/ELECTRA
https://github.com/google-research/electra

Transformers大集合
https://huggingface.co/transformers/index.html
https://github.com/huggingface/transformers

可视化BERTs预训练模型
https://github.com/jessevig/bertviz

知识蒸馏
https://github.com/airaria/TextBrewer

追一科技
https://github.com/ZhuiyiTechnology/simbert

GPT3预训练模型
https://github.com/openai/gpt-3

ACL 2020 的论文 《Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning》对BERT的MLM机制低效问题进行了改进
https://arxiv.org/abs/2004.08097 
https://github.com/joongbo/tta

基于bert的ocr识别结果校正
https://github.com/tiantian91091317/OCR-Corrector

性能媲美BERT，但参数量仅为1/300，这是谷歌最新的NLP模型--PRADO 
https://www.aclweb.org/anthology/D19-1506.pdf
改进PRADO的模型pQRNN 
https://github.com/tensorflow/models/tree/master/research/sequence_projection

神州泰岳AI研究院与中科院深圳先进技术研究院合作推出BERT-EMD：借助EMD实现多对多层映射的BERT压缩方法
论文：https://arxiv.org/abs/2010.06133
代码：https://github.com/lxk00/BERT-EMD
介绍：https://hub.baai.ac.cn/view/2768

2020年3月，哈工大讯飞联合实验室推出了中文ELECTRA预训练模型，ELECTRA提出了一套新的预训练框架，其中包含两个部分：Generator和Discriminator
http://github.com/ymcui/Chinese-ELECTRA

清华和阿里的paper：使用bert解决长文本
《CogLTX: Applying BERT to Long Texts》
论文链接：
http://keg.cs.tsinghua.edu.cn/jietang/publications/NIPS20-Ding-et-al-CogLTX.pdf
Github：
https://github.com/Sleepychord/CogLTX

《Adversarial Training for Large Neural Language Models》首次全面对「对抗预训练」进行了全面研究，建立了一种统一的、适配于语言模型的对抗训练框架——ALUM (Adversarial training for large neural LangUage Models)
论文下载地址：https://arxiv.org/pdf/2004.08994
论文开源地址：https://github.com/namisan/mt-dnn

《Rethinking Attention with Performers》---对Transformer中的self-Attention的改进
论文地址:https://www.aminer.cn/pub/5f75feb191e0111c1eb4dbb2/rethinking-attention-with-performers
论文代码：https://github.com/google-research/google-research/tree/master/performer

《ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning》实体和关系抽取的预训练模型
论文链接：https://arxiv.org/abs/2012.15022
开源链接：https://github.com/thunlp/ERICA

《2109Paradigm Shift in Natural Language Processing》
论文链接：https://arxiv.org/abs/2109.12575
项目网站：https://txsun1997.github.io/nlp-paradigm-shift/
Slides：https://txsun1997.github.io/slides/nlp-paradigm-shift.pdf

2021.09微软和英伟达联合发布的MT-NLG预训练模型《Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model》
https://github.com/NVIDIA/Megatron-LM
英伟达官网：https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/

微软NLG预训练模型：《BANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining》
论文地址：https://arxiv.org/abs/2012.15525
github地址：https://github.com/microsoft/BANG

《Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese》
论文链接：https://arxiv.org/pdf/2110.06696.pdf 
论文代码：https://github.com/Langboat/Mengzi

《DeepNet_ Scaling Transformers to 1,000 Layers》
