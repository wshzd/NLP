基于keras的轻量级bert
https://github.com/bojone/bert4keras

ALbert模型的实现
https://link.zhihu.com/?target=https%3A//github.com/brightmart/albert_zh

使用PaddlePaddle做词向量skip-gram
http://www.360doc.com/content/19/0816/04/46368139_855209381.shtml

bert for tf2.0
https://github.com/kpe/bert-for-tf2

adapter-bert的地址
https://github.com/google-research/adapter-bert/

bert作为服务应用
https://github.com/hanxiao/bert-as-service

Skip-Thought Vectors
https://arxiv.org/pdf/1506.06726.pdf

BioBERT: 用于生物医学文本挖掘的预先训练生物医学语言表示模型
fine-tuning代码
https://github.com/dmis-lab/biobert
BioBERT预训练模型权重下载
https://github.com/naver/biobert-pretrained
BioBERT论文解读
https://blog.csdn.net/devshilei/article/details/103887864

Bert可视化工具bertviz
https://github.com/jessevig/bertviz
https://blog.csdn.net/duan_zhihua/article/details/87388646

BERT论文合集
https://github.com/tomohideshibata/BERT-related-papers#domain-specific

全面剖析Transformer
https://jalammar.github.io/illustrated-transformer/

Transformer
https://arxiv.org/abs/1706.03762.pdf
tensorflow版本：https://github.com/tensorflow/tensor2tensor
pytorch版本：http://nlp.seas.harvard.edu/2018/04/03/attention.html

Transformer-XL
https://arxiv.org/pdf/1901.02860.pdf
https://github.com/kimiyoung/transformer-xl
https://www.lyrn.ai/2019/01/16/transformer-xl-sota-language-model/
https://blog.csdn.net/magical_bubble/article/details/89060213

Kashgari代码成果
https://kashgari.readthedocs.io/en/v1.1.1/embeddings/bert-embedding_v2.html

基于对抗学习的ELECTRA_TINY预训练模型
https://github.com/CLUEbenchmark/ELECTRA
https://github.com/google-research/electra

Transformers大集合
https://huggingface.co/transformers/index.html
https://github.com/huggingface/transformers

可视化BERTs预训练模型
https://github.com/jessevig/bertviz

知识蒸馏
https://github.com/airaria/TextBrewer

追一科技
https://github.com/ZhuiyiTechnology/simbert






